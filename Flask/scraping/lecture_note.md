## 스크래핑
- 스크래핑 != 크롤링
- 웹 크롤링 : 구글 검색엔진처럼 A 사이트를 방문해서 그 사이트에 있는 a 태그를 수집 ➡️ 여러 링크로 방문하는 태그 리스트 모아서 ➡️ 검색해야 할 사이트 목록(todo-list)을 만듦 ➡️ 무한 루프 / 거미줄(스파이더 웹)
- 웹 스크래핑 : 사이트 내에 있는 정보를 가져와서 활용 / 다른 사이트에 의존적인 경우 오래 갈 수 없음 ➡️ API 활용하는 것이 더 좋다 ➡️ 페이지 변동되어도 API 스펙 보장 해주면 변경되는 주기가 적음

## 고급 프로그래밍
1. 스크래핑을 해서 todo-list에 저장하는(1~100개 정도까지 큐잉이 가능함)
  - 잡을 수행하는 (worker-thread)를 생성... (5~10개)
2. 큐에 있는 잡을 읽어서 프로세싱해서 저장(celery - python 기반에 job queueing 라이브러리)
3. CRUD를 수행해서 2번에서 저장한 DB를 가져와서 사용자에게 보여줌